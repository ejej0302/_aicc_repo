{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 모델 적용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 SKT KoBERT \n",
    "\n",
    "\n",
    "&nbsp;&nbsp;  ■ 참고 사이트 : 1)[SKT KoBERT](https://github.com/SKTBrain/KoBERT) 홈페이지\n",
    "2) [Hugging Face 공유 모델 Model: monologg/kobert](https://huggingface.co/monologg/kobert)\n",
    "> **KoBERT 에 대한 간단한 소개** \\\n",
    "구글 BERT base multilingual cased의 한국어 성능 한계로 이를 보완하고자, 다량의 한국어 코퍼스를 기반으로 만들어진 한글 맞춤 버트 입니다. \\\n",
    "학습셋은 한국어 위키 (5M 문장, 54M 단어) + 한국어 뉴스 (20M 문장, 270M 단어)로 학습되었습니다. \\\n",
    "사전(Vocabulary) 크기 : 8,002 \\\n",
    "토크나이저 : 한글 위키 + 뉴스 텍스트 기반으로 학습한 토크나이저(SentencePiece) \\\n",
    "\n",
    "■ 학습시 시도했던 방법 3가지 :\n",
    "1) Hugging Face 공유 모델(.from_pretrained)을 활용한 방법으로 시도 \\\n",
    "2) SKT KoBERT 에서 공유한 예제 기반 \\\n",
    "3) 직접 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### (1) 환경 설치 및 라이브러리 임포트 \n",
    "- KoBERT 설치\n",
    "- [파이토치 설치 (설치환경별 코드 상이)](https://pytorch.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT 설치\n",
    "# git clone https://github.com/SKTBrain/KoBERT.git\n",
    "# cd KoBERT\n",
    "# pip install -r requirements.txt\n",
    "# pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# GPU 사용 지정\n",
    "if torch.cuda.is_available():    \n",
    "    # PyTorch 에게 GPU를 사용하라고 지시합니다.    \n",
    "    device = torch.device('cuda')\n",
    "#     print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "#     print('We will use the GPU:', torch.cuda.get_device_name(1))\n",
    "\n",
    "# If not... CPU 사용 지정\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### (2) 데이터 로딩 \n",
    "- 데이터 탐색 및 전처리 시 저장한 <code>train_data.tsv</code>, <code>test_data.tsv</code> 불러옵니다.\n",
    "- 각 데이터는 label(예측해야할 값)과 text(예측에 사용될 값)로 이루어져 있습니다.\n",
    "- 2.1 구글버트에서 로딩했던 방법과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences 개수: 34,396\n",
      "Test sentences 개수: 8,327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['고객님 정보 확인 불가 문의 안녕하세요 당일 고객님 분실정지 진행하려고 하는데 인입시 고객정보가 조회되지 않습니다 처리결과 값이 올바르지 않습니다 라고 나오면 정보조회의 기본정보 성명 생년월일등 가 확인되지 않습니다 해당 내용 확인 부탁 드립니다 감사합니다',\n",
       "  '긴급 tv m oss 결과보고시 기존바코드 확인안되어hds연동할수 없음 업무경로 데이터 국사 원주 회선번호 수리접수번호 상품정보 서비스계약번호 연락처 이윤범 현장 확인사항 수리오더로 결과보고시 연동시 기존바코드가 올라오지않아 교체업무 진행을 할 수 가 없습니다 완료보고 확인시 기존바코드에 빌트인 로 확인되며 단말원부확인시 댁내설치된기존바코드 확인시 사용 사용으로 되어 있습니다 긴급건으로 확인 및 조치 부탁 드립니다'],\n",
       " ['ASM34824', 'ASM31688'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset \n",
    "train = pd.read_csv(\"data/train_data.tsv\", sep ='\\t', header=0, encoding='utf-8', names=['label','text'])\n",
    "test = pd.read_csv(\"data/test_data.tsv\", sep ='\\t', header=0, encoding='utf-8', names=['label','text'])\n",
    "\n",
    "# sentences 개수\n",
    "print('Training sentences 개수: {:,}'.format(train.shape[0]))\n",
    "print('Test sentences 개수: {:,}'.format(test.shape[0]))\n",
    "\n",
    "# train/test 각각의 text 와 label을 리스트로 가져옵니다.\n",
    "train_texts = train.text.values.tolist()\n",
    "train_labels = train.label.values.tolist()\n",
    "test_texts = test.text.values.tolist()\n",
    "test_labels = test.label.values.tolist()\n",
    "\n",
    "# 데이터 모양 출력\n",
    "train_texts[:2] , train_labels[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### (3) 데이터 포맷 변경 \n",
    "1. label(Output : y) 정수화 - label 포맷을 str 문자에서 int 정수로 매핑 \n",
    "2. text(Input : x) 토큰화 - 데이터를 BERT를 학습 할 수 있는 형식으로 변환합니다. \\\n",
    "  . 문장별 토큰화 \\\n",
    "  . vocab 기반 정수 인덱스 매핑 & padding \\\n",
    "  . input text mask 적용 \\\n",
    "  (text 토큰화는 아래 3가지 모델 적용방식에서 따로 구현하였으니 참고바랍니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 개수 : 34396 \n",
      "Test Data 개수 : 8327\n",
      "\n",
      "▶ label 정수 인덱스화 예시 : ['ASM14326', 'ASM30034', 'ASM30014'] -> [6, 65, 58]\n"
     ]
    }
   ],
   "source": [
    "# 1. label(Output : y) 정수 인코딩 \n",
    "label_idx = {j:i for i,j in enumerate(sorted(set(test.label.values.tolist())))}  # dict{ 'ASM14261' : 0 , 'ASM14262' : 1 , ...} \n",
    "train_labels = [label_idx[i] for i in train.label.values.tolist()]\n",
    "test_labels = [label_idx[i] for i in test.label.values.tolist()]\n",
    "print('Train Data 개수 :',len(train_labels) , '\\nTest Data 개수 :',len(test_labels))\n",
    "print('\\n▶ label 정수 인덱스화 예시 :' , test.label.values.tolist()[:3], '->', test_labels[:3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### (4) 모델 학습 (3가지 방법)\n",
    "\n",
    "A. Hugging Face 공유 모델을 사용(.from_pretrained) \\\n",
    "B. SKT Kobert github 공개 예제 차용 \\\n",
    "C. kobert pytorch 구현 (from the scratch) \\\n",
    "C-2.(추가) .py 스크립트 파일을 이용한 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### A.Hugging Face 공유 모델을 사용 (.from_pretrained)\n",
    "- monologg/kobert의 토큰화 적용시 UNK 다수\n",
    "- 신뢰성 부족"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 원본 문장:  고객님 정보 확인 불가 문의 안녕하세요 당일 고객님 분실정지 진행하려고 하는데 인입시 고객정보가 조회되지 않습니다 처리결과 값이 올바르지 않습니다 라고 나오면 정보조회의 기본정보 성명 생년월일등 가 확인되지 않습니다 해당 내용 확인 부탁 드립니다 감사합니다\n",
      "\n",
      "토큰화 적용:  ['[UNK]', '정보', '확인', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '하는데', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '라고', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '가', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '확인', '[UNK]', '드립니다', '[UNK]']\n",
      "\n",
      "Token ids 적용:  [0, 7229, 7945, 0, 0, 0, 0, 0, 0, 0, 7795, 0, 0, 0, 0, 0, 0, 0, 0, 6004, 0, 0, 0, 0, 0, 5330, 0, 0, 0, 0, 7945, 0, 5925, 0]\n"
     ]
    }
   ],
   "source": [
    "# 2. text(Input : x) 토큰화\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer in Hugging Face share models version.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 예시) 원본 text를 출력 \n",
    "print('train data 원본 문장: ', train_texts[0])\n",
    "\n",
    "# 예시) 문장을 토큰화 적용 \n",
    "print('\\n토큰화 적용: ', tokenizer.tokenize(train_texts[0]))\n",
    "\n",
    "# 예시) 문장의 토큰을 token ids(정수)로 매핑한 결과 출력\n",
    "print('\\nToken ids 적용: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_texts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 결과 역시 좋지 않았음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 허깅페이스에 올려둔 kobert 관련 공유 모델(\"monologg/kobert\")을 적용해보니 모델의 안정성이 낮아 적용하지 않았습니다. \\\n",
    "> 위의 예시와 같이 다운받은 모델의 tokenizer를 확인해보면, **대다수의 문장의 토큰이 [UNK]**으로 정보가 손실되고 있음을 확인 할 수 있었습니다. \\\n",
    "> 허깅페이스의 공유모델은 skt에서 공식적으로 올린 버전은 없었고, 일반 개인이 kobert를 이용해 허깅페이스 버전으로 모델을 공유한 것으로 확인되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### B. SKT Kobert github 공개 예제 활용\n",
    "- 공개 예제를 기반으로 커스텀화 하여 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 264425,
     "status": "ok",
     "timestamp": 1600852298781,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "COxrxlPk1J6Q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bd5202f8-6b77-48be-9e8a-d9134063d05c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu101\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n",
      "\u001b[K     |███████████████████████████████▌| 834.1MB 1.3MB/s eta 0:00:09tcmalloc: large alloc 1147494400 bytes == 0x39894000 @  0x7fc5faa74615 0x591f47 0x4cc229 0x4cc38b 0x50a51c 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d\n",
      "\u001b[K     |████████████████████████████████| 846.0MB 21kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (1.18.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101) (2.23.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
      "Installing collected packages: graphviz, mxnet-cu101\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n",
      "Collecting gluonnlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 8.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.18.5)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588544 sha256=83630f27fe072027ce56b7ecf35205be767513eaa4e3d2af858282dcab03212a\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: gluonnlp\n",
      "Successfully installed gluonnlp-0.10.0\n",
      "Collecting sentencepiece==0.1.85\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 7.9MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.85\n",
      "Collecting transformers==2.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 5.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2.23.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (0.1.85)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 13.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.14.63)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1) (1.18.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.1.1) (2020.6.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.1.1) (0.16.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (1.17.63)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.1.1) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->transformers==2.1.1) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->transformers==2.1.1) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f388d8ddad84f7a7d1d43f43325f021d41065f00bbe3d9345b1f2ee37cbaf923\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 transformers-2.1.1\n",
      "Collecting torch==1.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
      "\u001b[K     |████████████████████████████████| 734.6MB 23kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.18.5)\n",
      "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 1.6.0+cu101\n",
      "    Uninstalling torch-1.6.0+cu101:\n",
      "      Successfully uninstalled torch-1.6.0+cu101\n",
      "Successfully installed torch-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu101\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece==0.1.85\n",
    "!pip install transformers==2.1.1\n",
    "!pip install torch==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 272330,
     "status": "ok",
     "timestamp": 1600852307737,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "mbe78bOm0H8w",
    "outputId": "b5b49c27-9d47-455b-d3cd-a24c3625f28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-wk2cz5rs\n",
      "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-wk2cz5rs\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kobert: filename=kobert-0.1.1-cp36-none-any.whl size=12825 sha256=724be3bc8f59370043f4eca70147e50fd1716b5fe66b83730f3328f1d64d851b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-85qegn8t/wheels/a2/b0/41/435ee4e918f91918be41529283c5ff86cd010f02e7525aecf3\n",
      "Successfully built kobert\n",
      "Installing collected packages: kobert\n",
      "Successfully installed kobert-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 301139,
     "status": "ok",
     "timestamp": 1600852337275,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "q2YFSHsg08aZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import WarmupLinearSchedule\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 356376,
     "status": "ok",
     "timestamp": 1600852393346,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "CFJaPjNf0H80",
    "outputId": "67f4a864-58c7-4878-d469-eaaf2207ed68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "[██████████████████████████████████████████████████]\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# 2. text(Input : x) 토큰화\n",
    "\n",
    "from transformers.optimization import WarmupLinearSchedule\n",
    "\n",
    "# 모델 \n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "    \n",
    "# 데이터 로드 \n",
    "dataset_train = nlp.data.TSVDataset('data/train_data.tsv', field_indices=[0,1], num_discard_samples=1, allow_missing=True)\n",
    "dataset_test = nlp.data.TSVDataset('data/test_data.tsv', field_indices=[0,1], num_discard_samples=1, allow_missing=True)\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1600852575427,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "SpTge2Io3r3z",
    "outputId": "610ce34c-44a6-464a-8767-68c577819f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences 개수: 34,396\n",
      "\n",
      "Test sentences 개수: 8,327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "import pandas as pd\n",
    "train = pd.read_csv(\"train_data.tsv\", sep ='\\t', header=0, encoding='utf-8', names=['label','text'])\n",
    "test = pd.read_csv(\"test_data.tsv\", sep ='\\t', header=0, encoding='utf-8', names=['label','text'])\n",
    "\n",
    "# sentences 개수\n",
    "print('Training sentences 개수: {:,}\\n'.format(train.shape[0]))\n",
    "print('Test sentences 개수: {:,}\\n'.format(test.shape[0]))\n",
    "\n",
    "# train/test 각각의 text 와 label을 리스트로 가져옵니다.\n",
    "train_texts = train.text.values.tolist()\n",
    "train_labels = train.label.values.tolist()\n",
    "\n",
    "test_texts = test.text.values.tolist()\n",
    "test_labels = test.label.values.tolist()\n",
    "\n",
    "label_dict = {j:i for i,j in enumerate(sorted(set(test.label.values.tolist())))}  # dict{ 'ASM14261' : 0 , 'ASM14262' : 1 , ...} \n",
    "train_labels = [label_dict[i] for i in train.label.values.tolist()]\n",
    "test_labels = [label_dict[i] for i in test.label.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1600852576389,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "c4nkjzMk0H82"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, labels, label_idx, sent_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i) for i in labels]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1600852578103,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "baFbUX8j0H85"
   },
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 256\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.1 #0\n",
    "num_epochs = 8\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21209,
     "status": "ok",
     "timestamp": 1600852599603,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "aARC5hMW0H88"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, train_labels, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, test_labels, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20467,
     "status": "ok",
     "timestamp": 1600852599604,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "7Rx1SnjU2PRR"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18521,
     "status": "ok",
     "timestamp": 1600852599606,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "Bt9UHnAsMU2c",
    "outputId": "b155f96d-fc90-433b-bdb0-d3c695a760f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17798,
     "status": "ok",
     "timestamp": 1600852599607,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "04Ndxr2dKHrj"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes= 183,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15706,
     "status": "ok",
     "timestamp": 1600852599607,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "2iPvZeKeMtS9"
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13750,
     "status": "ok",
     "timestamp": 1600852599609,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "G-Qy0-MSMwVN"
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "# 옵티마이저 \n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 스케줄러\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9947,
     "status": "ok",
     "timestamp": 1600852599611,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "ysLpHcKnM04Y"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■ 학습 시작\n",
    "- max_len: 256, lr = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c139c79acce44280bff309dfdc990fef",
      "1e13be931b764255ae1ba7f8b6e32269",
      "2ed35a8bb64d4135bb2e5a1136a25e15",
      "e7832f373ede48c3b00fbba5de8b73a6",
      "a514910bc30d45508a4d1731ee00e2c3",
      "b80e21c54ce94e2f8b82bfd774b2db69",
      "18f8dab1b01c4344aaa353c5e4713e87",
      "fcee2c84ae7549798824b112a4b879da",
      "dedd03570a1f48ad84a5cd65f2977ad7",
      "83e64157925945a598d1aff961b305b7",
      "67f70dc8bf7d465a8a6e7396214db91f",
      "bcbe4c2b4bc1455c9152b19938643258",
      "69c8dd036b2b4acea79247eb0ce54064",
      "d2fff53b725641739a4efd98cb7616e7",
      "b3a40e26106941a79efdf7ff21398dab",
      "999c55b529a1449fb272025c4f39540c",
      "6dbcb61782c34738963e956b9528c004",
      "dba943cf38eb42159ea8bc0efc42ce21",
      "2dc6d93441724f089cba4e1b3377aaed",
      "40126410303e488b87612aa7bdfd5514",
      "2bfb2aa49b4e4926a7170be8f8ecbac7",
      "99407431085f45d0a0cce60f894929b1",
      "346429fd43b24cd9a5f884ed044342ea",
      "64b017041ee54335924277aa712217a4",
      "985d3a8bfb774fbc92e3e1c0852b7fb2",
      "8582434f53d045aa93b386ce54a4b74d",
      "fb93a96582b344f38dd419f4aaed3c4b",
      "4a660ef1b89a447f8486abc87b9ce352",
      "312e60c003ad442fa38d670f1d86474b",
      "3ab62efca17b433ab57156614b3a99bb",
      "e1c8a2285c0d4a84b5c573ab20fc9963",
      "b49d94f07df747cdac60f2a161d36e9e",
      "0bde12db71384f6ea9bd673aefe5b613",
      "9db0969d63f7480ca49de2e5c0791e38",
      "4b367b0bd8fe4a149080f03a8e9feda7",
      "4913665113a9457d865bdb556baf8d1f",
      "33765db933814749ad7a730623326bff",
      "b1d9a6da189c44fab10186395c40a187",
      "ad872b608c794bc1999251fbcd4b801c",
      "74483f3cc62647b58974da9584e7257b",
      "87ca6f34bcdc427383804d135349272c",
      "060be35fa66442339427133feb25989c",
      "7de28a0151ee443eb453fca9a52c708a",
      "410f886188fa4330956931c135c36dad",
      "f10ba5d76f884d66ab0509f069b97c7a",
      "113b28c2382c4ce392ef0f16f649de66",
      "aa9f7a8ef2ce4bf7873c1f5bc1a6ca00",
      "91c277b6038f4e9fb44c779ebb477d81",
      "06bf758b8be24c12bd71c8f421e977de",
      "76e5bcb626114545ade4487ed74656d2",
      "348577f790c4481a80fd3a51180a1fbe",
      "70dec36fe2db4355819152e67cb482b6",
      "f3893f70a08242ab9cd5a413e2f07803",
      "5cc8ed9e829b4c228cbe2f899314bd87",
      "d82b0bb1c56e4c4db21a5fc23aa3043f",
      "2ecdcd0db5c141bbaba7c0c559f0e127",
      "1111239e38d847eb887faeeec21f91b5",
      "fc5b1a5cb91441c4bf1b6ec1ed2f1e2f",
      "98173ce6bc40403380bc946e328dbbbe",
      "dee9fafec8134407b76cb5bbb8e16a2f",
      "58653c7c967e4fc9a7f3777884994124",
      "edf9d1b8ae73474abeb117af292bba53",
      "7b0b879dbef04662a6d72ce8f4cc794e",
      "30294e4f566c4923ae3658739162777b",
      "cf1e9991552443c08f55d40bb42556b1",
      "02dbdd4e51484085a27985811ef5c7f3",
      "ebd498decdf94507ab72e538489bdead",
      "a07f05ffc0ed449ab93e30036f2c6590",
      "d84257516f544a36b97e81527e8ee51b",
      "bf7eb0c17c8d418bb96f6560f7726492",
      "fcdcbf56eb2843fd817d1584879b43e2",
      "9743f4bc00744f36804416c0ef7b342c",
      "7ada8255955c4a479d96ea0a4bcf0c04",
      "a0f85084077e4e8b91cc093410930011",
      "1c182e69d72f49fab34389bc48e70fd0",
      "9ec823438489486fb6134e2b7b9b8680",
      "ae358bf2122e4d159790f087b229a52a",
      "0f26c4343bd34d1ab41523dafc627859",
      "7b0de47435624ae0b0586b5068009403",
      "e57d16c6cb85470e9687498383d7b52e",
      "7a7c1489bba646e9ae678eeb61659db0",
      "dd5627de54514a829143dea942942330",
      "6b334e90672545b380f6eede94bd2c1e",
      "58a0c51e9dcf4cafaf8fb247ffa82a4e",
      "9020491e285349349d8b8dc95816adab",
      "ae93167e224544a79216aae7501a00c2",
      "2da1acb949494b4e8e1cd0d3ca2d6869",
      "4d9bf5093ef941edbc8efc614856c1f5",
      "6d5a81e8741b4b90a51c0b322c2a8535",
      "bb7e713bf1614d049f487f3a93fea807",
      "7630518e73f945089f19d752fcfd71c3",
      "0fc08c206cc247d999b1dae892c9d043",
      "111a692f09e44203b7255692c9ad591f",
      "45768b5e5bd645a6903a912fd97ae7ae",
      "7848b5faf51442a9a05d91e7a6a9577a",
      "99bc0b3b0e9d4364b7142a4d6d2bd446",
      "74610a18be1448a7ac4e2c8d0c9dc331",
      "2c576a7325464b78a290d6b38aacef7a",
      "40d650dd0d7a494fb49fefc967705cfb",
      "559c966066db45ed9f7e01fdfd24ec70",
      "fa1897b255ee4552a6708c87dacb90a2",
      "b852a3291db3443596d481d3161c1a0d",
      "180b5f9f00ad413bafb8cc97b3fd4595",
      "f4c59c1ad0dd45d2b1baf2753f1f2023",
      "5934201183834be3b42a77c5b5766eac",
      "c679a6b1414740bd85d9bb8d2bad22cf",
      "c8620e06962342538d6082ad8b9e21e0",
      "d9cc99e011ef4f8485579403b23803b7",
      "62d0540a5aa04cc19304d2fa50014bc1",
      "dd162a3f73354775b7a6164545473189",
      "69ce727bb0394d0d9982b0142023a878",
      "59f24c6d78814bbe81b06cc0066dba9b",
      "256a1d9bab324174a319db3cc85bcb6c",
      "d9a39b8eb5614b5ab3a3e2d71f69a464",
      "00ce690eee5f410c93e2ac1ead1b72cd",
      "310e673810a74b5db16912392b88a28d",
      "e7f1c3f0ef7442728c42ff4888840eac",
      "0d4a78f3d64749ee9671e213fe53df47",
      "57256cc2b5bb4b26b8dd0fd11133fb03",
      "dd789ef5b19e476d8a8266febbd1efe2",
      "e4bfbec1cdda424ba39ea71b329e6f65",
      "a3b7c26d66984184ab2a2acac366c78a",
      "a131900224ad4747958ecede2dcdff8c",
      "5f5a4a68ebc54866ae485d8725ad170f",
      "36656c19e53d4a4f96e100d52e385c25",
      "192695e52204450e953916fc20ab7a65",
      "a81223a2a24c4425a9ef4a01485ca842",
      "1e890cce3844450594dc640e68294feb"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15398092,
     "status": "ok",
     "timestamp": 1600867988021,
     "user": {
      "displayName": "EJ park",
      "photoUrl": "",
      "userId": "16745642217864885000"
     },
     "user_tz": -540
    },
    "id": "s8cdaAWIM2Zq",
    "outputId": "f6ee8352-3393-4803-9afa-dc2335480268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c139c79acce44280bff309dfdc990fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 5.303109645843506 train acc 0.0\n",
      "epoch 1 batch id 201 loss 5.0639777183532715 train acc 0.02767412935323383\n",
      "epoch 1 batch id 401 loss 4.778959274291992 train acc 0.04239401496259352\n",
      "epoch 1 batch id 601 loss 4.11456298828125 train acc 0.05178868552412646\n",
      "epoch 1 batch id 801 loss 3.341724395751953 train acc 0.07997815230961298\n",
      "epoch 1 batch id 1001 loss 2.7731356620788574 train acc 0.12793456543456544\n",
      "epoch 1 batch id 1201 loss 2.7290821075439453 train acc 0.17938176519567028\n",
      "epoch 1 batch id 1401 loss 2.468221426010132 train acc 0.2221627408993576\n",
      "epoch 1 batch id 1601 loss 1.7572476863861084 train acc 0.25975952529668955\n",
      "epoch 1 batch id 1801 loss 1.4746654033660889 train acc 0.2940380344253193\n",
      "epoch 1 batch id 2001 loss 1.746311068534851 train acc 0.3232446276861569\n",
      "\n",
      "epoch 1 train acc 0.34375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedd03570a1f48ad84a5cd65f2977ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 test acc 0.6062517137373183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbcb61782c34738963e956b9528c004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.67597234249115 train acc 0.5625\n",
      "epoch 2 batch id 201 loss 1.0161962509155273 train acc 0.5718283582089553\n",
      "epoch 2 batch id 401 loss 2.393324136734009 train acc 0.593360349127182\n",
      "epoch 2 batch id 601 loss 1.1267873048782349 train acc 0.5992096505823628\n",
      "epoch 2 batch id 801 loss 1.1827679872512817 train acc 0.6113451935081149\n",
      "epoch 2 batch id 1001 loss 2.041337013244629 train acc 0.6221903096903096\n",
      "epoch 2 batch id 1201 loss 1.0758068561553955 train acc 0.6343671940049959\n",
      "epoch 2 batch id 1401 loss 1.851233720779419 train acc 0.645521056388294\n",
      "epoch 2 batch id 1601 loss 0.96855628490448 train acc 0.6546299188007495\n",
      "epoch 2 batch id 1801 loss 0.41522830724716187 train acc 0.6634855635757912\n",
      "epoch 2 batch id 2001 loss 0.8762131929397583 train acc 0.6714455272363818\n",
      "\n",
      "epoch 2 train acc 0.6765116279069767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985d3a8bfb774fbc92e3e1c0852b7fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 test acc 0.7033006580751302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bde12db71384f6ea9bd673aefe5b613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 1.3271163702011108 train acc 0.625\n",
      "epoch 3 batch id 201 loss 0.4705163240432739 train acc 0.7154850746268657\n",
      "epoch 3 batch id 401 loss 1.6134333610534668 train acc 0.7227244389027432\n",
      "epoch 3 batch id 601 loss 0.5181348323822021 train acc 0.721089850249584\n",
      "epoch 3 batch id 801 loss 0.660610020160675 train acc 0.7272159800249688\n",
      "epoch 3 batch id 1001 loss 1.505534291267395 train acc 0.7328921078921079\n",
      "epoch 3 batch id 1201 loss 0.5381229519844055 train acc 0.739748126561199\n",
      "epoch 3 batch id 1401 loss 1.1948156356811523 train acc 0.7465649536045682\n",
      "epoch 3 batch id 1601 loss 0.5854792594909668 train acc 0.7524594003747658\n",
      "epoch 3 batch id 1801 loss 0.4388090968132019 train acc 0.7570099944475291\n",
      "epoch 3 batch id 2001 loss 0.2723481357097626 train acc 0.7615567216391804\n",
      "\n",
      "epoch 3 train acc 0.7647383720930233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ca6f34bcdc427383804d135349272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 test acc 0.7417226487523992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bf758b8be24c12bd71c8f421e977de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 1.2908886671066284 train acc 0.75\n",
      "epoch 4 batch id 201 loss 0.44723010063171387 train acc 0.7820273631840796\n",
      "epoch 4 batch id 401 loss 1.234215497970581 train acc 0.7835099750623441\n",
      "epoch 4 batch id 601 loss 0.4354294240474701 train acc 0.7792221297836939\n",
      "epoch 4 batch id 801 loss 0.49687978625297546 train acc 0.783941947565543\n",
      "epoch 4 batch id 1001 loss 1.2532260417938232 train acc 0.7900224775224776\n",
      "epoch 4 batch id 1201 loss 0.23885023593902588 train acc 0.7955349708576187\n",
      "epoch 4 batch id 1401 loss 1.1808233261108398 train acc 0.8010795860099929\n",
      "epoch 4 batch id 1601 loss 0.4593297839164734 train acc 0.8059025608994379\n",
      "epoch 4 batch id 1801 loss 0.23374906182289124 train acc 0.8098972792892837\n",
      "epoch 4 batch id 2001 loss 0.21739551424980164 train acc 0.8135932033983009\n",
      "\n",
      "epoch 4 train acc 0.8155232558139535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1111239e38d847eb887faeeec21f91b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 test acc 0.7612763915547025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1e9991552443c08f55d40bb42556b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.8204097151756287 train acc 0.8125\n",
      "epoch 5 batch id 201 loss 0.398654967546463 train acc 0.8280472636815921\n",
      "epoch 5 batch id 401 loss 0.9888365864753723 train acc 0.8308915211970075\n",
      "epoch 5 batch id 601 loss 0.32603320479393005 train acc 0.8299708818635607\n",
      "epoch 5 batch id 801 loss 0.707271158695221 train acc 0.8320848938826467\n",
      "epoch 5 batch id 1001 loss 1.1512913703918457 train acc 0.8361013986013986\n",
      "epoch 5 batch id 1201 loss 0.17350754141807556 train acc 0.8409138218151541\n",
      "epoch 5 batch id 1401 loss 0.9455651640892029 train acc 0.8456905781584583\n",
      "epoch 5 batch id 1601 loss 0.2592124044895172 train acc 0.84946908182386\n",
      "epoch 5 batch id 1801 loss 0.21805977821350098 train acc 0.8531024430871738\n",
      "epoch 5 batch id 2001 loss 0.1412857174873352 train acc 0.8566654172913544\n",
      "\n",
      "epoch 5 train acc 0.8587015503875969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ada8255955c4a479d96ea0a4bcf0c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 test acc 0.7642754318618042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7c1489bba646e9ae678eeb61659db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.6702744364738464 train acc 0.8125\n",
      "epoch 6 batch id 201 loss 0.20723655819892883 train acc 0.8610074626865671\n",
      "epoch 6 batch id 401 loss 0.6920619010925293 train acc 0.8644014962593516\n",
      "epoch 6 batch id 601 loss 0.3106262981891632 train acc 0.8631447587354409\n",
      "epoch 6 batch id 801 loss 0.31842437386512756 train acc 0.8658707865168539\n",
      "epoch 6 batch id 1001 loss 0.7936205267906189 train acc 0.8692557442557443\n",
      "epoch 6 batch id 1201 loss 0.06561380624771118 train acc 0.8742714404662781\n",
      "epoch 6 batch id 1401 loss 0.9733357429504395 train acc 0.8785242683797287\n",
      "epoch 6 batch id 1601 loss 0.16807642579078674 train acc 0.8826514678326046\n",
      "epoch 6 batch id 1801 loss 0.028620421886444092 train acc 0.8854802887284842\n",
      "epoch 6 batch id 2001 loss 0.06731796264648438 train acc 0.8889930034982508\n",
      "\n",
      "epoch 6 train acc 0.8899709302325581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5a81e8741b4b90a51c0b322c2a8535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 6 test acc 0.7785508637236085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74610a18be1448a7ac4e2c8d0c9dc331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.4677696228027344 train acc 0.8125\n",
      "epoch 7 batch id 201 loss 0.2717874050140381 train acc 0.8908582089552238\n",
      "epoch 7 batch id 401 loss 0.6238974928855896 train acc 0.8952618453865336\n",
      "epoch 7 batch id 601 loss 0.3166370987892151 train acc 0.8950707154742097\n",
      "epoch 7 batch id 801 loss 0.09493833780288696 train acc 0.8968476903870163\n",
      "epoch 7 batch id 1001 loss 0.7508553862571716 train acc 0.8980394605394605\n",
      "epoch 7 batch id 1201 loss 0.02687174081802368 train acc 0.9012801831806828\n",
      "epoch 7 batch id 1401 loss 0.7920633554458618 train acc 0.9052462526766595\n",
      "epoch 7 batch id 1601 loss 0.052518486976623535 train acc 0.9084556527170519\n",
      "epoch 7 batch id 1801 loss 0.025860190391540527 train acc 0.9104664075513603\n",
      "epoch 7 batch id 2001 loss 0.034074246883392334 train acc 0.9132621189405298\n",
      "\n",
      "epoch 7 train acc 0.9141957364341085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5934201183834be3b42a77c5b5766eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 test acc 0.7863483685220729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256a1d9bab324174a319db3cc85bcb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2150.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.4070279002189636 train acc 0.9375\n",
      "epoch 8 batch id 201 loss 0.02105635404586792 train acc 0.9104477611940298\n",
      "epoch 8 batch id 401 loss 0.40893927216529846 train acc 0.9141209476309227\n",
      "epoch 8 batch id 601 loss 0.25954002141952515 train acc 0.9137895174708819\n",
      "epoch 8 batch id 801 loss 0.09685277938842773 train acc 0.9158863920099876\n",
      "epoch 8 batch id 1001 loss 0.38326725363731384 train acc 0.9177072927072927\n",
      "epoch 8 batch id 1201 loss 0.027375519275665283 train acc 0.920587010824313\n",
      "epoch 8 batch id 1401 loss 0.9562413096427917 train acc 0.923313704496788\n",
      "epoch 8 batch id 1601 loss 0.0733366310596466 train acc 0.9262960649594004\n",
      "epoch 8 batch id 1801 loss 0.009888052940368652 train acc 0.9279219877845641\n",
      "epoch 8 batch id 2001 loss 0.07057502865791321 train acc 0.9302848575712144\n",
      "\n",
      "epoch 8 train acc 0.9304554263565892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bfbec1cdda424ba39ea71b329e6f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 8 test acc 0.7905470249520153\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_acc_list = []\n",
    "    test_loss_list = []\n",
    "    test_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "        test_loss += loss\n",
    "    avg_loss = test_loss / (batch_id+1)\n",
    "    avg_acc = test_acc / (batch_id+1)\n",
    "    print(\"epoch {} test loss {}\".format(e+1, test_loss / (batch_id+1)))\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    test_acc_list.append(avg_acc)\n",
    "    if avg_loss <= min(test_loss_list):\n",
    "        torch.save(model.state_dict(), 'kobert_256_16.pt')\n",
    "    test_loss_list.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 공개 예제 버전으로 learning_rate =  5e-5 일때 <code>test acc : 0.7905</code> 로 산출되었습니다.\n",
    "(참고로, lr =  2e-5 일 때 0.7816 로 산출) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### C. kobert pytorch 구현 (from the scratch)\n",
    "- pytorch를 기반 으로 from the scratch 로 학습 모듈을 구현 하였습니다.\n",
    "- 전체 문장을 max_len으로 자르고 input_ids를 구하는 방법에서(구글버트 버전) -> 스텝(step)마다 학습 시 필요한 배치 데이터에 input_ids, mask를 그때그때 적용하는 방식으로 진행하였습니다. \\\n",
    "    (이 방법은 메모리 부담을 줄이는 면에서 긍정적) \n",
    "- tqdm 패키지의 pbar 사용하여 실시간 학습 경과 구현 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "seed_val = 2020\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from torch import nn\n",
    "\n",
    "# 1) 버트 분류 모델 구현 \n",
    "class BertClassification(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super(BertClassification, self).__init__()\n",
    "        self.encoder, _  = get_pytorch_kobert_model()\n",
    "        self.classification_head = nn.Linear(768,n_class)\n",
    "        nn.init.normal_(self.classification_head.weight,std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, input_mask, token_type_ids):\n",
    "        sequence_output, pooled_output = self.encoder(input_ids, input_mask, token_type_ids)\n",
    "        return self.classification_head(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "import torch\n",
    "\n",
    "# 2) 배치 단위별로 indexing, padding, mask 진행   \n",
    "class Batchfier:\n",
    "    def __init__(self, padding_idx = 0):\n",
    "        tok_path = get_tokenizer()\n",
    "        # 2. text(Input : x) 토큰화\n",
    "        self.tokenizer = SentencepieceTokenizer(tok_path) # SentencepieceTokenizer 적용\n",
    "        _, self.vocab = get_pytorch_kobert_model() \n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def batchfy(self, texts, labels=None):\n",
    "        \"\"\"\n",
    "        > param texts : lists of texts\n",
    "        > param labels : lists of indexed labels\n",
    "        > return : indexed texts, masks, token_type_ids\n",
    "        \"\"\"\n",
    "        def index_one(text):\n",
    "            # 문장앞에 cls token을 넣어줍니다.(분류 목적)\n",
    "            indexed = [self.vocab[self.vocab.cls_token]] + self.vocab[self.tokenizer(text)]\n",
    "            return indexed[:MAX_LEN]   \n",
    "            \n",
    "        indexed = [index_one(text) for text in texts]\n",
    "        padded = self.pad_indexed(indexed)\n",
    "        masks = self.get_masks(indexed)\n",
    "        types = self.get_types(indexed)\n",
    "        \n",
    "        if labels:\n",
    "            return padded, masks, types, torch.LongTensor(labels)\n",
    "        else:\n",
    "            return padded, masks, types\n",
    "\n",
    "    def pad_indexed(self, indexed):\n",
    "        padded = [torch.LongTensor(text) for text in indexed]\n",
    "        padded = torch.nn.utils.rnn.pad_sequence(padded, batch_first=True, padding_value=self.padding_idx)\n",
    "        return padded\n",
    "\n",
    "    def get_masks(self, indexed):\n",
    "        masks = [torch.LongTensor([1 for _ in range(len(text))]) for text in indexed]\n",
    "        masks = torch.nn.utils.rnn.pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "        return masks\n",
    "\n",
    "    def get_types(self, indexed):\n",
    "        bs, l = len(indexed), max(map(len,indexed))\n",
    "        return torch.zeros((bs,l), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 3) Trainer 클래스로 training과 test(validation) 함수 구현 \n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler, batch_size=16 ): \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.batchfier = Batchfier()\n",
    "        self.batch_size = batch_size\n",
    "        self.cur_idx = 0\n",
    "        self.criteria = nn.CrossEntropyLoss()\n",
    "        self.scheduler = scheduler \n",
    "        \n",
    "    def get_acc(self, logits, y):\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total = y.size(0)\n",
    "        correct = (predicted == y).sum().item()\n",
    "        return correct, total\n",
    "\n",
    "    def get_batch(self, texts, labels):\n",
    "        if self.cur_idx > len(texts):\n",
    "            return None\n",
    "        # 배치사이즈 만큼 자르기\n",
    "        batch_texts = texts[self.cur_idx:self.cur_idx + self.batch_size]\n",
    "        batch_labels = labels[self.cur_idx:self.cur_idx + self.batch_size]\n",
    "        self.cur_idx += self.batch_size\n",
    "        return self.batchfier.batchfy(batch_texts, batch_labels)    \n",
    "    \n",
    "    def train_epochs(self, texts, labels):\n",
    "        self.cur_idx = 0\n",
    "        self.model.train()\n",
    "        self.model.zero_grad()\n",
    "        pbar = tqdm()\n",
    "        tot_loss = 0\n",
    "        tot_correct = 0\n",
    "        total_n = 0\n",
    "        total_step = 0\n",
    "        # 랜덤셔플적용 # \n",
    "        combine  = list(zip(texts,labels))\n",
    "        random.shuffle(combine)\n",
    "        texts,labels = zip(*combine)\n",
    "        while True:\n",
    "            res = self.get_batch(texts,labels) \n",
    "            if not res:\n",
    "                break\n",
    "            x, masks, token_index, y = res\n",
    "            x = x.to(device)\n",
    "            masks = masks.to(device)\n",
    "            token_index = token_index.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = self.model(x, masks, token_index)\n",
    "            loss = self.criteria(logits, y)\n",
    "            correct, n = self.get_acc(logits,y)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0) # # \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step() # # \n",
    "            self.model.zero_grad()\n",
    "            tot_correct += correct\n",
    "            total_n += n\n",
    "            tot_loss+=loss\n",
    "            total_step +=1\n",
    "            pbar.set_description(\n",
    "                \"training loss : %f training acc : %f step : %f\" % (\n",
    "                    tot_loss / total_step, tot_correct / total_n, total_step) )\n",
    "        pbar.close()\n",
    "\n",
    "    def test_epochs(self, texts, labels):\n",
    "        self.cur_idx = 0\n",
    "        self.model.eval()\n",
    "        pbar = tqdm()\n",
    "        tot_loss = 0\n",
    "        tot_correct = 0\n",
    "        total_n = 0\n",
    "        total_step = 0\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                res = self.get_batch(texts, labels)\n",
    "                if not res:\n",
    "                    break\n",
    "                x, masks, token_index, y = res\n",
    "                x = x.to(device)\n",
    "                masks = masks.to(device)\n",
    "                token_index = token_index.to(device)\n",
    "                y = y.to(device)\n",
    "                logits = self.model(x, masks, token_index)\n",
    "                loss = self.criteria(logits, y)\n",
    "                correct, n = self.get_acc(logits, y)\n",
    "                tot_correct += correct\n",
    "                total_n += n\n",
    "                tot_loss += loss\n",
    "                total_step += 1               \n",
    "                pbar.set_description(\n",
    "                    \"test loss : %f test acc : %f step : %f\" % (\n",
    "                        tot_loss / total_step, tot_correct / total_n, total_step))\n",
    "                epoch_loss = tot_loss/total_step\n",
    "                epoch_acc = tot_correct/total_n\n",
    "        pbar.close()\n",
    "        return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "# max_len 최대로 실험 \n",
    "epochs = 12\n",
    "MAX_LEN = 400 \n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# 모델 가져오기\n",
    "def get_model(lr,train_texts):\n",
    "    model = BertClassification(n_class=183).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps = 1e-8) \n",
    "    total_steps = len(train_texts)/BATCH_SIZE * epochs  \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps) \n",
    "    return model, optimizer , scheduler\n",
    "\n",
    "# 메인 함수 \n",
    "def main(train_texts, train_labels, test_texts, test_labels):\n",
    "    lrs = [2.5e-5]\n",
    "    for lrss in lrs:\n",
    "        print('--------{}-----'.format(lrss))\n",
    "        model, optimizer, scheduler = get_model(lr=lrss, train_texts=train_texts) # 초기화\n",
    "        trainer = Trainer(model, optimizer, scheduler, batch_size=BATCH_SIZE)\n",
    "        loss_list = []\n",
    "        for i in range(epochs): \n",
    "            trainer.train_epochs(train_texts, train_labels)\n",
    "            epoch_loss, epoch_acc = trainer.test_epochs(test_texts, test_labels)\n",
    "#             if epoch_loss <= min(loss_list):\n",
    "#                 torch.save(model.state_dict(), f'kobert_{lrss}_{MAX_LEN}_{BATCH_SIZE}.pt') \n",
    "#             loss_list.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train text set 개수 : 34396 \n",
      "Train labels 개수 : 34396\n",
      "Test text set 개수 : 8327 \n",
      "Test labels 개수 : 8327\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터 체크\n",
    "print('Train text set 개수 :',len(train_texts) , '\\nTrain labels 개수 :',len(train_labels))\n",
    "print('Test text set 개수 :',len(test_texts) , '\\nTest labels 개수 :',len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "■ 학습 시작\n",
    "- 직접 구현한 kobert 프로세스에서는 lr=2.5e-5 , max_len=400 일때 상대적으로 좋은 성능을 보였습니다. \n",
    "- 최종 성능은 best_loss 기준 시 0.7772 , best_acc 기준 0.7918 로 산출되었습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2.5e-05-----\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss : 2.292726 training acc : 0.508489 step : 4300.000000: : 0it [11:18, ?it/s]\n",
      "test loss : 1.482641 test acc : 0.664585 step : 1041.000000: : 0it [00:46, ?it/s]\n",
      "training loss : 1.160836 training acc : 0.729591 step : 4300.000000: : 0it [11:04, ?it/s]\n",
      "test loss : 1.175307 test acc : 0.732317 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.858477 training acc : 0.793057 step : 4300.000000: : 0it [10:28, ?it/s]\n",
      "test loss : 1.053249 test acc : 0.753212 step : 1041.000000: : 0it [00:40, ?it/s]\n",
      "training loss : 0.665886 training acc : 0.835533 step : 4300.000000: : 0it [11:26, ?it/s]\n",
      "test loss : 1.041169 test acc : 0.762459 step : 1041.000000: : 0it [00:42, ?it/s]\n",
      "training loss : 0.518977 training acc : 0.872253 step : 4300.000000: : 0it [11:27, ?it/s]\n",
      "test loss : 1.036964 test acc : 0.777231 step : 1041.000000: : 0it [00:41, ?it/s]\n",
      "training loss : 0.403604 training acc : 0.901587 step : 4300.000000: : 0it [11:22, ?it/s]\n",
      "test loss : 1.066042 test acc : 0.782635 step : 1041.000000: : 0it [00:45, ?it/s]\n",
      "training loss : 0.322861 training acc : 0.923477 step : 3579.000000: : 0it [09:57, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 학습 시작! \n",
    "main(train_texts = train_texts, train_labels = train_labels, test_texts = test_texts, test_labels = test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2.5e-05-----\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss : 2.387952 training acc : 0.471712 step : 4300.000000: : 0it [10:41, ?it/s]\n",
      "test loss : 1.577102 test acc : 0.631200 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 1.235698 training acc : 0.709559 step : 4300.000000: : 0it [10:31, ?it/s]\n",
      "test loss : 1.231580 test acc : 0.718146 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.921678 training acc : 0.775584 step : 4300.000000: : 0it [10:45, ?it/s]\n",
      "test loss : 1.067951 test acc : 0.750691 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.727624 training acc : 0.819979 step : 4300.000000: : 0it [10:47, ?it/s]\n",
      "test loss : 1.069843 test acc : 0.762459 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.595471 training acc : 0.853006 step : 4300.000000: : 0it [14:53, ?it/s]\n",
      "test loss : 1.076355 test acc : 0.772427 step : 1041.000000: : 0it [01:16, ?it/s]\n",
      "training loss : 0.486731 training acc : 0.880742 step : 4300.000000: : 0it [14:14, ?it/s]\n",
      "test loss : 1.121378 test acc : 0.777231 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.393003 training acc : 0.906239 step : 4300.000000: : 0it [10:45, ?it/s]\n",
      "test loss : 1.170452 test acc : 0.783596 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.315990 training acc : 0.926183 step : 4300.000000: : 0it [10:51, ?it/s]\n",
      "test loss : 1.262373 test acc : 0.787078 step : 1041.000000: : 0it [00:39, ?it/s]\n",
      "training loss : 0.250296 training acc : 0.941127 step : 4300.000000: : 0it [18:03, ?it/s]\n",
      "test loss : 1.266111 test acc : 0.790080 step : 1041.000000: : 0it [01:15, ?it/s]\n",
      "training loss : 0.197052 training acc : 0.954530 step : 4300.000000: : 0it [18:22, ?it/s]\n",
      "test loss : 1.300026 test acc : 0.788880 step : 1041.000000: : 0it [01:16, ?it/s]\n",
      "training loss : 0.157946 training acc : 0.963833 step : 4300.000000: : 0it [18:12, ?it/s]\n",
      "test loss : 1.334017 test acc : 0.791161 step : 1041.000000: : 0it [01:16, ?it/s]\n",
      "training loss : 0.126877 training acc : 0.970316 step : 4300.000000: : 0it [18:22, ?it/s]\n",
      "test loss : 1.332749 test acc : 0.791882 step : 1041.000000: : 0it [01:15, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작! \n",
    "main(train_texts = train_texts, train_labels = train_labels, test_texts = test_texts, test_labels = test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 직접 구현한 버전으로 <code>test acc : 0.7918</code> 까지 산출되었습니다. \\\n",
    "> 과제의 목표인 acc 0.80 까지는 어려워보여, kobert를 버리고 다른 버트를 통해 학습 및 실험 하고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### C-2.(추가) 각각의 기능별 클래스를 .py 스크립트 파일로 저장 후 bash로 학습 \n",
    "- C. kobert pytorch 구현 (from the scratch)에서 만든 클래스를 .py 파일로 기능 분리 구현하였습니다.\n",
    "- bash script를 통해 학습을 진행하였습니다.\n",
    "- (관련 파일은 ./kobert 폴더 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------2.5e-05--------\n",
      "--------epodch 1-----\n",
      "training loss : 2.675329 training acc : 0.441418 step : 2150.000000 \n",
      "test loss : 1.596798 test acc : 0.652936 step : 521.000000 \n",
      "--------epodch 2-----\n",
      "training loss : 1.294857 training acc : 0.708396 step : 2150.000000 \n",
      "test loss : 1.199180 test acc : 0.727513 step : 521.000000 \n",
      "--------epodch 3-----\n",
      "training loss : 0.936986 training acc : 0.776893 step : 2150.000000 \n",
      "test loss : 1.050810 test acc : 0.755494 step : 521.000000 \n",
      "--------epodch 4-----\n",
      "training loss : 0.727372 training acc : 0.822043 step : 2150.000000 \n",
      "test loss : 0.985938 test acc : 0.763540 step : 521.000000 \n",
      "--------epodch 5-----\n",
      "training loss : 0.577965 training acc : 0.853966 step : 2150.000000 \n",
      "test loss : 0.970217 test acc : 0.769064 step : 521.000000 \n",
      "--------epodch 6-----\n",
      "training loss : 0.462415 training acc : 0.883591 step : 2150.000000 \n",
      "test loss : 0.998379 test acc : 0.773628 step : 521.000000 \n",
      "--------epodch 7-----\n",
      "training loss : 0.372296 training acc : 0.907053 step : 2150.000000 \n",
      "test loss : 1.018963 test acc : 0.779512 step : 521.000000 \n",
      "--------epodch 8-----\n",
      "training loss : 0.299176 training acc : 0.926997 step : 2150.000000 \n",
      "test loss : 1.046978 test acc : 0.785757 step : 521.000000 \n",
      "--------epodch 9-----\n",
      "training loss : 0.242330 training acc : 0.942726 step : 2150.000000 \n",
      "test loss : 1.073961 test acc : 0.786358 step : 521.000000 \n",
      "--------epodch 10-----\n",
      "training loss : 0.203242 training acc : 0.952960 step : 2150.000000 \n",
      "test loss : 1.075652 test acc : 0.790080 step : 521.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/kobert_train_1.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### (10) lessons learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구글 버트를 적용할 때 보다 kobert를 사용할 때, 정확도가 약 0.5% 정도 더 높아짐을 확인 하였습니다.\n",
    "- kobert에서 사용한 하이퍼 파라메터는 구글버트와 동일하게 max_len = 256, learning_rate = 5e-5 에서 좋은 결과가 있음을 확인하였습니다.\n",
    "- pytorch를 이용하여 training 방법을 연구하고 적용해 보았습니다.\n",
    "- notebook 내 코드를 구현 방식에서 -> .py 스크립트 파일로 기능/모듈별 코드를 구현하여 bash로 학습하는 법을 익혔습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작성자 : 박은진"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
